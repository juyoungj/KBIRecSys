{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Collaborative Filtering_Logistic Regression.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# # Collaborative Filtering\n",
    "\n",
    "# 1. Connecting DB(oracle) to Python\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# # Collaborative Filtering Not Using Package\n",
    "\n",
    "# ## 은행별 분류전 과정명, 수료구분 preprocessing\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# sql query에서 수료구분 부분을 수정한 데이터\n",
    "df_TrainMerged_new = pd.read_csv('수강정보_수정본.csv', encoding = 'ms949', low_memory=False)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#Generally Mendatory Courses Exclusion\n",
    "wordsToExclude = ['펀드투자상담사 등록교육', '펀드투자상담사등록', '채무증권 투자권유 실무', \n",
    "                  '사원기관 HRD과정', 'ATD 사원기관 연수담당 책임자 과정', '펀드투자상담사(부동산,파생상품)',\n",
    "                 '펀드투자상담사(부동산)', '펀드투자상담사(파생상품)', '펀드투자상담사 자격시험대비',\n",
    "                 '펀드투자권유자문인력 사전교육(주말)(펀드투자자보호 교육)']\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "addWordsToExc = [\"Banking Business English\",\n",
    "\"Big Data Analytics : Machine Learning\",\n",
    "\"Impact of Information Technology on Finance\",\n",
    "\"KBI지식콘텐츠 서비스(테스트용)\",\n",
    "\"SC TEST\",\n",
    "\"Understanding FX and Money Markets\",\n",
    "\"Understanding of Derivatives - Forward/Futures\",\n",
    "\"Understanding of Investment Banking - DCM\",\n",
    "\"격파! 파생상품투자권유자문인력\",\n",
    "\"대출상담사 등록교육\",\n",
    "\"대출상담사 등록교육(신)\",\n",
    "\"대출상담사 정기교육\",\n",
    "\"대출상담사 정기교육(신)\",\n",
    "\"매경 TEST\",\n",
    "\"베트남 금융연수원 연수\",\n",
    "\"베트남 은행구조조정 및 부실채권정리 과정\",\n",
    "\"은퇴설계전문가-Core 등록교육\",\n",
    "\"채무증권 투자권유 실무\",\n",
    "\"채무증권 투자권유 실무 보수교육\",\n",
    "\"FP보수\",\n",
    "\"PRMTP(CIFO, Chicago)\",\n",
    "\"TEST\",\n",
    "\"간접투자상품판매\",\n",
    "\"간접투자상품판매\",\n",
    "\"간접투자상품판매 보수교육\",\n",
    "\"간접투자상품판매 보수교육(예산용)\",\n",
    "\"간접투자상품판매(단가차액에따른금액)\",\n",
    "\"간접투자상품판매실무(단가차액에따른금액)\",\n",
    "\"광주지역 간접투자상품판매실무\",\n",
    "\"국제금융역 보수\",\n",
    "\"기술신보테스트과정\",\n",
    "\"기타과정\",\n",
    "\"대구지역 간접투자상품판매실무\",\n",
    "\"대출심사역보수\",\n",
    "\"부동산펀드투자상담사\",\n",
    "\"부산지역 간접투자상품판매실무\",\n",
    "\"수탁\",\n",
    "\"신용분석사 보수\",\n",
    "\"여신심사역 보수\",\n",
    "\"외국환업무t\",\n",
    "\"자금운용역보수\",\n",
    "\"자산관리사(FP) 보수\",\n",
    "\"정신교육(남자)\",\n",
    "\"정신교육(여자)\",\n",
    "\"증권펀드투자상담사\",\n",
    "\"증권펀드투자상담사 보수교육\",\n",
    "\"증권펀드투자상담사 보수교육\",\n",
    "\"집합 모의\",\n",
    "\"투자상담관리인력 등록교육\",\n",
    "\"투자상담관리인력 보수교육\",\n",
    "\"파생상품투자권유자문인력 전문성교육\",\n",
    "\"파생상품투자상담사 보수교육\",\n",
    "\"파생상품펀드투자상담사\",\n",
    "\"펀드투자상담사 등록교육\",\n",
    "\"펀드투자상담사 보수교육(하)\",\n",
    "\"평가BMT\",\n",
    "\"모바일평가 검수\",\n",
    "\"JB 리더스클럽\",\n",
    "\"펀드투자권유자문인력 전문성교육\",\n",
    "\"펀드투자권유자문인력 투자자보호\",\n",
    "\"창의적인 금융인을 꿈꾸는 청소년을 위한 금융교육\",\n",
    "\"펀드투자상담사(부동산,파생상품)\",\n",
    "\"FinTech Business Models\"]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "wordsExTotalList = wordsToExclude + addWordsToExc\n",
    "wordsExTotalList\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "wordList_기관 = ['IBK', '우리은행', '농협', '농협은행', '부산은행', \n",
    "               'KEB하나은행', '하나은행', 'KEB/HANA', 'HANA/KEB', 'KEB', 'Hanabank', '외환은행', 'NH','수협',\n",
    "              '대구은행', '광주은행', '산업은행', '경남은행', 'KDB', '스탠다드차타드', '전북', '씨티', 'Citi', '제주',\n",
    "              '수출입은행', '신용보증기금', '기술보증기금', '제일은행', '상호저축은행', '신한은행',\n",
    "              '국민은행', 'KB', 'L1', 'L2', 'L3', 'BDC', 'BNK', 'CSR', 'DGB', 'KDB', 'KJB', 'PFC',\n",
    "              'SC', 'Teller', 'Woori', '국민주택기금', '금융감독원', '기술신보', '기업은행', '대한투자신탁',\n",
    "              '부은 사관학교', '부은금융사관학교', '삼성생명', '새마을금고', '서울신탁은행', '서울은행', '성업공사',\n",
    "              '신보', '예금보험공사', '우리 FPM', '우리론컨설턴트', '은행감독원', '재경부', '전문건설공제조합', '전북, 광주은행',\n",
    "              '전북은행', '부은 금융사관학교', '제주은행', '조흥은행', '케이뱅크', '하나금융그룹', '한국선물거래',\n",
    "              '한국신용정보원', '한미은행', '한빛은행', '현대투자신탁', '회원수협', '중소기업진흥공단']\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "df_TrainMerged_new_edited = df_TrainMerged_new[['D_OPN_YY','I_TRAN_TP_NAME', 'I_EXE_ST', 'O_REG', 'N_COUR', 'I_TRAN_NAME', 'N_CSTM']]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "df_TrainMerged_new.I_TRAN_TP_NAME.value_counts(normalize=True)\n",
    "\n",
    "\n",
    "# ### Data Preprocessing\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "def datapreprocessing(df, year1, wordsToExList, instituteList):\n",
    "    df_post2010 = df[df['D_OPN_YY']>=year1]\n",
    "    df_post2010 = df_post2010[df_post2010['D_OPN_YY']<2019]\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_TP_NAME']=='정규연수']\n",
    "    df_post2010.dropna(subset=['N_COUR'], inplace = True)\n",
    "    #미수료, 퇴교된 과정 제외\n",
    "    df_post2010 = df_post2010[df_post2010['I_EXE_ST']!='미수료']\n",
    "    df_post2010 = df_post2010[df_post2010['I_EXE_ST']!='퇴교']\n",
    "    #집합, 통신, 사이버만\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']!='일반(기타)연수']\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']!='자격실무교육']\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']!='해외연수']\n",
    "    #보수교육 제외\n",
    "    df_post2010 = df_post2010[~df_post2010['N_COUR'].str.contains('보수')]\n",
    "    \n",
    "    df_post2010_NF = df_post2010[['O_REG', 'D_OPN_YY', 'N_COUR', 'I_TRAN_NAME', 'N_CSTM']]\n",
    "    \n",
    "    \n",
    "    for word in wordsToExList:\n",
    "        df_post2010_NF = df_post2010_NF[df_post2010_NF['N_COUR']!=word]\n",
    "        df_post2010_NF = df_post2010_NF[~df_post2010_NF['N_COUR'].str.contains(word)]\n",
    "    \n",
    "    for word in instituteList:\n",
    "        df_post2010_NF = df_post2010_NF[~df_post2010_NF['N_COUR'].str.contains(word)]\n",
    "        \n",
    "    #수료했음에도 재수강이력 제외\n",
    "#     df_post2010_NF = df_post2010_NF.drop_duplicates(subset=['O_REG', 'N_COUR'], keep = 'first')\n",
    "    # 5개 이상의 수업을 들은 수강생들만 남김.\n",
    "#     df_post2010_NF = df_post2010_NF.groupby('O_REG').filter(lambda x: len(x) > 4)\n",
    "    #9999로 시작하는 test id 제외\n",
    "    df_post2010_NF = df_post2010_NF[~df_post2010_NF['O_REG'].str.contains(\"99999\")]\n",
    "    \n",
    "    \n",
    "    return df_post2010_NF\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def datapreprocessing집합(df, year1, wordsToExList, instituteList):\n",
    "    df_post2010 = df[df['D_OPN_YY']>=year1]\n",
    "    df_post2010 = df_post2010[df_post2010['D_OPN_YY']<2019]\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_TP_NAME']=='정규연수']\n",
    "    df_post2010.dropna(subset=['N_COUR'], inplace = True)\n",
    "    #미수료, 퇴교된 과정 제외\n",
    "    df_post2010 = df_post2010[df_post2010['I_EXE_ST']!='미수료']\n",
    "    df_post2010 = df_post2010[df_post2010['I_EXE_ST']!='퇴교']\n",
    "    #집합 만\n",
    "    df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']=='집합연수']\n",
    "#     df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']!='자격실무교육']\n",
    "#     df_post2010 = df_post2010[df_post2010['I_TRAN_NAME']!='해외연수']\n",
    "    #보수교육 제외\n",
    "    df_post2010 = df_post2010[~df_post2010['N_COUR'].str.contains('보수')]\n",
    "    \n",
    "    df_post2010_NF = df_post2010[['O_REG', 'D_OPN_YY', 'N_COUR', 'I_TRAN_NAME', 'N_CSTM']]\n",
    "    \n",
    "    \n",
    "    for word in wordsToExList:\n",
    "        df_post2010_NF = df_post2010_NF[df_post2010_NF['N_COUR']!=word]\n",
    "        df_post2010_NF = df_post2010_NF[~df_post2010_NF['N_COUR'].str.contains(word)]\n",
    "    \n",
    "    for word in instituteList:\n",
    "        df_post2010_NF = df_post2010_NF[~df_post2010_NF['N_COUR'].str.contains(word)]\n",
    "        \n",
    "    #수료했음에도 재수강이력 제외\n",
    "    df_post2010_NF = df_post2010_NF.drop_duplicates(subset=['O_REG', 'N_COUR'], keep = 'first')\n",
    "    # 5개 이상의 수업을 들은 수강생들만 남김.\n",
    "    df_post2010_NF = df_post2010_NF.groupby('O_REG').filter(lambda x: len(x) > 4)\n",
    "    #9999로 시작하는 test id 제외\n",
    "    df_post2010_NF = df_post2010_NF[~df_post2010_NF['O_REG'].str.contains(\"99999\")]\n",
    "    \n",
    "    \n",
    "    return df_post2010_NF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfdf = datapreprocessing(df_TrainMerged_new_edited, 2010, wordsExTotalList, wordList_기관)\n",
    "dfdf_집합 = datapreprocessing집합(df_TrainMerged_new, 2010, wordsExTotalList, wordList_기관)\n",
    "dfdf_2012 = datapreprocessing(df_TrainMerged_new, 2012, wordsExTotalList, wordList_기관)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "exceptionList = ['신용분석기초', '신용분석기초(야)', '회계원리와 재무제표 작성', '회계원리와 재무제표 분석', '회계원리와 재무제표 작성(부산)']\n",
    "\n",
    "# In[16]:\n",
    "dfCourseNameChanges = pd.read_excel('과정변경이력.xlsx', encoding = 'ms949')\n",
    "\n",
    "# In[17]:\n",
    "def convertCourseNames(df, df_namechange):\n",
    "    \n",
    "    #exception 신용분석기초 & 집합연수\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i].N_COUR == '신용분석기초' and df.iloc[i].I_TRAN_NAME == '집합연수':\n",
    "            df.iloc[i].N_COUR = '회계원리와 재무제표 작성'\n",
    "            \n",
    "#     courseNameChanges = df_namechange[df_namechange.N_COUR != df_namechange.UNIQUE_NAME]\n",
    "    courseNameChanges_2 = df_namechange.set_index('N_COUR')\n",
    "    courseNameChanges_2 = courseNameChanges_2.loc[~courseNameChanges_2.index.duplicated(keep='first')]\n",
    "\n",
    "    \n",
    "    \n",
    "    oldCourseName = courseNameChanges_2.index\n",
    "    oldNameList = oldCourseName.tolist()\n",
    "    \n",
    "    uniqueNameList = courseNameChanges_2.UNIQUE_NAME.tolist()\n",
    "    \n",
    "    namesDict = {}\n",
    "    for i in range(len(oldNameList)):\n",
    "        namesDict[oldNameList[i]] = uniqueNameList[i]\n",
    "        \n",
    "    allCourseNameSet = set(df['N_COUR'])\n",
    "    necessaryCourseNameSet = set(oldNameList)\n",
    "    \n",
    "    exceptionCourseNameSet = allCourseNameSet - necessaryCourseNameSet\n",
    "    exceptionCourseNameList = list(exceptionCourseNameSet)\n",
    "    \n",
    "    for course in oldNameList:\n",
    "        df['N_COUR'] = df['N_COUR'].replace(course, namesDict[course])\n",
    "    \n",
    "    for course in exceptionCourseNameList:\n",
    "        df = df[df['N_COUR']!=course]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "df_namechanges = convertCourseNames(dfdf, dfCourseNameChanges)\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "df_집합 = convertCourseNames(dfdf_집합, dfCourseNameChanges)\n",
    "\n",
    "\n",
    "# In[233]:\n",
    "\n",
    "df_2012 = convertCourseNames(dfdf_2012, dfCourseNameChanges)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "def preprocessing2(df):\n",
    "    #(1) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(1\\)\",\"\")\n",
    "    #(비) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(비\\)\",\"\")\n",
    "    #(부산), (광주) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(부산\\)\",\"\")\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(광주\\)\",\"\")\n",
    "    #(금토) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(금토\\) \",\"\")\n",
    "    #(B) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(B\\)\",\"\")\n",
    "    #(구), (통), (사) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(구\\)\",\"\")\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(통\\)\",\"\")\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(사\\)\",\"\")\n",
    "    #(부제:..) 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(부제.*\\)\",\"\")\n",
    "    #주말 제거\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(주말\\) \",\"\")\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"주말 \",\"\")\n",
    "    df['N_COUR'] = df['N_COUR'].str.replace(r\"\\(주말\\)\",\"\")\n",
    "    \n",
    "    df['N_COUR'] = df['N_COUR'].str.rstrip()\n",
    "    df['N_COUR'] = df['N_COUR'].str.lstrip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "df_processing2 = preprocessing2(df_namechanges)\n",
    "\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "df_집합 = preprocessing2(df_집합)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "df_집합.I_TRAN_NAME.unique()\n",
    "\n",
    "\n",
    "# In[234]:\n",
    "\n",
    "df_2012 = preprocessing2(df_2012)\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "def preprocessing3(df, wordsToExList):\n",
    "    \n",
    "    for word in wordsToExList:\n",
    "        df = df[df['N_COUR']!=word]\n",
    "        df = df[~df['N_COUR'].str.contains(word)]\n",
    "\n",
    "\n",
    "    #수료했음에도 재수강이력 제외\n",
    "    df = df.drop_duplicates(subset=['O_REG', 'N_COUR'], keep = 'first')\n",
    "    # 5개 이상의 수업을 들은 수강생들만 남김.\n",
    "#     df = df.groupby('O_REG').filter(lambda x: len(x) > 4)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "df_processing3 = preprocessing3(df_processing2, wordsExTotalList)\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "df_processing3.shape\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "df_집합 = preprocessing3(df_집합, wordsExTotalList)\n",
    "\n",
    "\n",
    "# In[235]:\n",
    "df_2012 = preprocessing3(df_2012, wordsExTotalList)\n",
    "\n",
    "\n",
    "# df_processing3.to_excel('datapreprocessed.xlsx')\n",
    "\n",
    "# df_processing3.shape\n",
    "\n",
    "# In[201]:\n",
    "\n",
    "\n",
    "df_processing3 = pd.read_excel('datapreprocessed.xlsx', encoding = \"ms949\", index_col=0)\n",
    "\n",
    "\n",
    "# In[202]:\n",
    "\n",
    "\n",
    "df_processing3.shape\n",
    "\n",
    "\n",
    "# #개인정보 데이터\n",
    "# df_PI = pd.read_csv('Personal_InfoData.csv', encoding = \"ms949\", low_memory=False)\n",
    "\n",
    "# df_PI_NEW = df_PI[['N_CSTM', 'I_DMND_NAME']]\n",
    "\n",
    "# df_PI_NEW = df_PI_NEW.drop_duplicates()\n",
    "\n",
    "# df_PI_NEW.shape\n",
    "\n",
    "# df_PI_NEW.to_excel('InstDist.xlsx')\n",
    "\n",
    "# In[179]:\n",
    "\n",
    "\n",
    "df_Inst = pd.read_excel('InstDist.xlsx', encoding = \"ms949\")\n",
    "\n",
    "\n",
    "# In[180]:\n",
    "\n",
    "\n",
    "df_Inst.shape\n",
    "\n",
    "\n",
    "# In[181]:\n",
    "\n",
    "\n",
    "비사원기관List = np.unique(df_Inst[df_Inst['I_DMND_NAME']=='비사원기관'].N_CSTM).tolist()\n",
    "\n",
    "\n",
    "# In[183]:\n",
    "\n",
    "\n",
    "사원기관List = np.unique(df_Inst[df_Inst['I_DMND_NAME']=='사원기관'].N_CSTM).tolist()\n",
    "\n",
    "\n",
    "# In[203]:\n",
    "\n",
    "\n",
    "df_processing3_사원 = df_processing3[df_processing3['N_CSTM'].isin(사원기관List)]\n",
    "\n",
    "\n",
    "# In[212]:\n",
    "\n",
    "\n",
    "df_processing3_사원_CF = df_processing3_사원[['O_REG', 'N_COUR']]\n",
    "\n",
    "\n",
    "# In[213]:\n",
    "\n",
    "\n",
    "df_processing3_사원_CF_over5 = df_processing3_사원_CF.groupby('O_REG').filter(lambda x: len(x) > 4)\n",
    "\n",
    "\n",
    "# In[214]:\n",
    "\n",
    "\n",
    "df_processing3_사원_CF_over5.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.N_CSTM.value_counts()\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "def segmentByInst(df, inst):\n",
    "    df = df[df['N_CSTM']==inst]\n",
    "    return df\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "df_부산 = segmentByInst(df, '부산은행')\n",
    "df_국민 = segmentByInst(df, '국민은행')\n",
    "df_KEB = segmentByInst(df, 'KEB하나은행')\n",
    "df_신한 = segmentByInst(df, '신한은행')\n",
    "df_개인 = segmentByInst(df, '개인고객')\n",
    "\n",
    "\n",
    "# ## 사전사후과정 정리\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#사전사후과정\n",
    "coursePreReq = pd.read_excel('사전_사후_과정.xlsx', encoding = \"ms949\")\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "coursePreReq = coursePreReq[['후과정', '전과정']]\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "coursePreReq_unique = coursePreReq.drop_duplicates(subset=['후과정', '전과정'], keep = 'first')\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "coursePreReq_unique.head()\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "coursePreReq_Ser = coursePreReq_unique.groupby('후과정')['전과정'].apply(lambda x: x.tolist())\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "type(coursePreReq_Ser)\n",
    "\n",
    "\n",
    "# ## Alternating Least Squares(Matrix Factorization)\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "\n",
    "import random\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# In[157]:\n",
    "\n",
    "\n",
    "df = df_new_사원\n",
    "\n",
    "\n",
    "# In[158]:\n",
    "\n",
    "\n",
    "data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "data_items = data_mat.reset_index(drop=True)\n",
    "data_sparse = sparse.csr_matrix(data_items)\n",
    "\n",
    "\n",
    "# In[165]:\n",
    "\n",
    "\n",
    "data_index = data_mat.reset_index()\n",
    "data_index\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[160]:\n",
    "\n",
    "\n",
    "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "    \n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "    \n",
    "    #Precompute I and lambda * I\n",
    "#     X_I = sparse.eye(user_size)\n",
    "#     Y_I = sparse.eye(item_size)\n",
    "    \n",
    "    I = sparse.eye(features)\n",
    "    lI = lambda_val * I\n",
    "    \n",
    "    # Start main loop. For each iteration we first compute X and then Y\n",
    "    for i in range(iterations):\n",
    "#         print ('iteration %d of %d' % (i+1, iterations))\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "\n",
    "            # Get the user row.\n",
    "#             u_row = confidence[u,:].toarray() \n",
    "\n",
    "            # Calculate the binary preference p(u)\n",
    "#             p_u = u_row.copy()\n",
    "#             p_u[p_u != 0] = 1.0\n",
    "            u_row = sparse_data[u, :].toarray()\n",
    "    \n",
    "            p_u = u_row.copy()\n",
    "\n",
    "            # Calculate Cu and Cu - I\n",
    "#             CuI = sparse.diags(u_row, [0])\n",
    "#             Cu = CuI + Y_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "#             yT_CuI_y = Y.T.dot(CuI).dot(Y)\n",
    "            yT_pu = Y.T.dot(p_u.T)\n",
    "            X[u] = spsolve(yTy + lI, yT_pu)\n",
    "\n",
    "    \n",
    "        for i in range(item_size):\n",
    "\n",
    "            # Get the item column and transpose it.\n",
    "            i_row = sparse_data[:,i].T.toarray()\n",
    "\n",
    "            # Calculate the binary preference p(i)\n",
    "            p_i = i_row.copy()\n",
    "#             p_i[p_i != 0] = 1.0\n",
    "\n",
    "            # Calculate Ci and Ci - I\n",
    "#             CiI = sparse.diags(i_row, [0])\n",
    "#             Ci = CiI + X_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "#             xT_CiI_x = X.T.dot(CiI).dot(X)\n",
    "            xT_pi = X.T.dot(p_i.T)\n",
    "            Y[i] = spsolve(xTx + lI, xT_pi)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def nonzeros(m, row):\n",
    "    for index in xrange(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "      \n",
    "      \n",
    "def implicit_als_cg(Cui, features=20, iterations=20, lambda_val=0.1):\n",
    "    user_size, item_size = Cui.shape\n",
    "\n",
    "    X = np.random.rand(user_size, features) * 0.01\n",
    "    Y = np.random.rand(item_size, features) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "    for iteration in xrange(iterations):\n",
    "        print 'iteration %d of %d' % (iteration+1, iterations)\n",
    "        least_squares_cg(Cui, X, Y, lambda_val)\n",
    "        least_squares_cg(Ciu, Y, X, lambda_val)\n",
    "    \n",
    "    return sparse.csr_matrix(X), sparse.csr_matrix(Y)\n",
    "\n",
    "\n",
    "def least_squares_cg(Cui, X, Y, lambda_val, cg_steps=3):\n",
    "    users, features = X.shape\n",
    "    \n",
    "    YtY = Y.T.dot(Y) + lambda_val * np.eye(features)\n",
    "\n",
    "    for u in xrange(users):\n",
    "\n",
    "        x = X[u]\n",
    "        r = -YtY.dot(x)\n",
    "\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "        p = r.copy()\n",
    "        rsold = r.dot(r)\n",
    "\n",
    "        for it in xrange(cg_steps):\n",
    "            Ap = YtY.dot(p)\n",
    "            for i, confidence in nonzeros(Cui, u):\n",
    "                Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "\n",
    "            alpha = rsold / p.dot(Ap)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "\n",
    "            rsnew = r.dot(r)\n",
    "            p = r + (rsnew / rsold) * p\n",
    "            rsold = rsnew\n",
    "\n",
    "        X[u] = x\n",
    "\n",
    "alpha_val = 15\n",
    "conf_data = (data_sparse * alpha_val).astype('double')\n",
    "user_vecs, item_vecs = implicit_als_cg(conf_data, iterations=20, features=20)\n",
    "\n",
    "\n",
    "# In[161]:\n",
    "\n",
    "\n",
    "user_vecs, item_vecs = implicit_als(data_sparse, iterations=20, features=20, alpha_val=40)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "user_vecs.toarray().shape\n",
    "\n",
    "\n",
    "# In[263]:\n",
    "\n",
    "\n",
    "def makeRecDictionaryMF(userList):\n",
    "    # indexList = []\n",
    "    userLikesDict_MF = {}\n",
    "    recommenDict_MF = {}\n",
    "\n",
    "    for user in userList:\n",
    "        index = data_index[data_index.O_REG == user].index.values[0]\n",
    "        user_interactions = data_sparse[index,:].toarray()\n",
    "\n",
    "        # We don't want to recommend items the user has consumed. So let's\n",
    "        # set them all to 0 and the unknowns to 1.\n",
    "        user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "        user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "        # This is where we calculate the recommendation by taking the \n",
    "        # dot-product of the user vectors with the item vectors.\n",
    "        rec_vector = user_vecs[index,:].dot(item_vecs.T).toarray()\n",
    "\n",
    "        # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "        min_max = MinMaxScaler()\n",
    "        rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "        recommend_vector = user_interactions*rec_vector_scaled\n",
    "\n",
    "        # Get all the artist indices in order of recommendations (descending) and\n",
    "        # select only the top \"num_items\" items. \n",
    "        item_idx = np.argsort(recommend_vector)[::-1][:10]\n",
    "\n",
    "        knownlikesindex = np.where(recommend_vector == 0)\n",
    "        known_user_likes = data_items.columns.values[knownlikesindex]\n",
    "\n",
    "        userLikesDict_MF[user] = known_user_likes.tolist()\n",
    "        \n",
    "        for i in range(len(coursePreReq_Ser.index)):\n",
    "            if(coursePreReq_Ser.index[i] in known_user_likes.tolist()): \n",
    "                recSeries = pd.Series(index = data_mat.columns.values[item_idx]).drop(coursePreReq_Ser[i], errors = 'ignore').index.values\n",
    "        \n",
    "        recommenDict_MF[user] = recSeries.tolist()\n",
    "\n",
    "    dfRec_MF = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in recommenDict_MF.items() ])).T\n",
    "    userLikesSeries = pd.Series(userLikesDict_MF)\n",
    "    dfRec_MF['userLikes'] = userLikesSeries\n",
    "        \n",
    "    return dfRec_MF\n",
    "\n",
    "\n",
    "# In[264]:\n",
    "\n",
    "\n",
    "dfRec_MF = makeRecDictionaryMF(userList)\n",
    "\n",
    "\n",
    "# In[265]:\n",
    "\n",
    "\n",
    "dfRec_MF.to_excel(\"MF_사원_2010.xlsx\")\n",
    "\n",
    "\n",
    "# Sampling Users\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "def sampleUserList(df, nsamples, seed):\n",
    "    sampleUsersList = df.O_REG.sample(n=nsamples, random_state=seed).values.tolist()\n",
    "    return sampleUsersList\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "sampleUsersList = sampleUserList(df_new_사원_집합, 50, 2)\n",
    "\n",
    "\n",
    "# ## item-based recommendations\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity(data_items):\n",
    "    \"\"\"Calculate the column-wise cosine similarity for a sparse\n",
    "    matrix. Return a new dataframe matrix with similarities.\n",
    "    \"\"\"\n",
    "    data_sparse = sparse.csr_matrix(data_items)\n",
    "    similarities = cosine_similarity(data_sparse.transpose())\n",
    "    sim = pd.DataFrame(data=similarities, index= data_items.columns, columns= data_items.columns)\n",
    "    return sim\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "def makeRecDictionaryItemBased(usersList, df, df_items, df_neighbours, data_matrix, topN):\n",
    "    userLikesDict_itembased_nei = {}\n",
    "    recommenDict_itembased_nei = {}\n",
    "    for user in usersList:\n",
    "        user_index = df[df.O_REG == user].index.tolist()[0] # Get the frame index\n",
    "\n",
    "        known_user_likes = df_items.loc[user_index]\n",
    "        known_user_likes = known_user_likes[known_user_likes >0].index.values\n",
    "\n",
    "        most_similar_to_likes = df_neighbours.loc[known_user_likes]\n",
    "        similar_list = most_similar_to_likes.values.tolist()\n",
    "        similar_list = list(set([item for sublist in similar_list for item in sublist]))\n",
    "        neighbourhood = data_matrix[similar_list].loc[similar_list]\n",
    "\n",
    "        user_vector = df_items.loc[user_index].loc[similar_list]\n",
    "\n",
    "        score = neighbourhood.dot(user_vector).div(neighbourhood.sum(axis=1))\n",
    "        score = score.drop(known_user_likes)\n",
    "        \n",
    "#         print(type(score))\n",
    "\n",
    "#         recommenDict_itembased_nei[user] = score.nlargest(10).index.tolist()\n",
    "        known_user_list = known_user_likes.tolist()\n",
    "        \n",
    "        for i in range(len(coursePreReq_Ser.index)):\n",
    "            if(coursePreReq_Ser.index[i] in known_user_list):\n",
    "                score = score.drop(coursePreReq_Ser[i], errors = 'ignore')\n",
    "\n",
    "#         if('자금세탁방지 핵심요원(전문)' in known_user_list):\n",
    "#             score = score.drop('자금세탁방지 핵심요원(기초)')\n",
    "#         if('여신법률' in known_user_list):\n",
    "#             score = score.drop('여신법률(담보관리)기초')\n",
    "    \n",
    "        userLikesDict_itembased_nei[user] = known_user_likes.tolist()\n",
    "        recommenDict_itembased_nei[user] = score.nlargest(topN).index.tolist()\n",
    "    \n",
    "    \n",
    "    dfRec_itembased_nei = pd.DataFrame(recommenDict_itembased_nei).T\n",
    "    userLikesSeries_nei = pd.Series(userLikesDict_itembased_nei)\n",
    "    dfRec_itembased_nei['userLikes'] = userLikesSeries_nei\n",
    "    \n",
    "    return dfRec_itembased_nei\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "def makeRecDictionaryItemBased빈칸(usersList, df, df_items, df_neighbours, data_matrix):\n",
    "    userLikesDict_itembased_nei = {}\n",
    "    recommenDict_itembased_nei = {}\n",
    "    for user in usersList:\n",
    "        user_index = df[df.O_REG == user].index.tolist()[0] # Get the frame index\n",
    "\n",
    "        known_user_likes = df_items.loc[user_index]\n",
    "        known_user_likes = known_user_likes[known_user_likes >0].index.values\n",
    "\n",
    "        most_similar_to_likes = df_neighbours.loc[known_user_likes]\n",
    "        similar_list = most_similar_to_likes.values.tolist()\n",
    "        similar_list = list(set([item for sublist in similar_list for item in sublist]))\n",
    "        neighbourhood = data_matrix[similar_list].loc[similar_list]\n",
    "\n",
    "        user_vector = df_items.loc[user_index].loc[similar_list]\n",
    "\n",
    "        score = neighbourhood.dot(user_vector).div(neighbourhood.sum(axis=1))\n",
    "        score = score.drop(known_user_likes)\n",
    "        \n",
    "#         print(type(score))\n",
    "\n",
    "#         recommenDict_itembased_nei[user] = score.nlargest(10).index.tolist()\n",
    "        known_user_list = known_user_likes.tolist()\n",
    "        \n",
    "#         for i in range(len(coursePreReq_Ser.index)):\n",
    "#             if(coursePreReq_Ser.index[i] in known_user_list):\n",
    "#                 score = score.drop(coursePreReq_Ser[i], errors = 'ignore')\n",
    "\n",
    "#         if('자금세탁방지 핵심요원(전문)' in known_user_list):\n",
    "#             score = score.drop('자금세탁방지 핵심요원(기초)')\n",
    "#         if('여신법률' in known_user_list):\n",
    "#             score = score.drop('여신법률(담보관리)기초')\n",
    "    \n",
    "        userLikesDict_itembased_nei[user] = known_user_likes.tolist()\n",
    "        score_top10 = score.nlargest(10)\n",
    "        \n",
    "        for i in range(len(coursePreReq_Ser.index)):\n",
    "            if(coursePreReq_Ser.index[i] in known_user_list):\n",
    "                score_top10 = score_top10.drop(coursePreReq_Ser[i], errors = 'ignore')\n",
    "        \n",
    "        recommenDict_itembased_nei[user] = score_top10.index.tolist()\n",
    "    \n",
    "    \n",
    "    dfRec_itembased_nei = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in recommenDict_itembased_nei.items() ])).T\n",
    "    userLikesSeries_nei = pd.Series(userLikesDict_itembased_nei)\n",
    "    dfRec_itembased_nei['userLikes'] = userLikesSeries_nei\n",
    "    \n",
    "    return dfRec_itembased_nei\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "def recommendItemBased(df, usersList, neighbour):\n",
    "\n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    data = data_mat.reset_index()\n",
    "#     data_mat.reset_index(inplace = True)\n",
    "#     data_mat_train, data_mat_test = train_test_split(data, test_size = 0.33, random_state=42)\n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "#     data_items_train, data_items_test = train_test_split(data_items, test_size = 0.20, random_state=42)\n",
    "    #normalize\n",
    "    magnitude_total = np.sqrt(np.square(data_items).sum(axis=1))\n",
    "    data_items = data_items.divide(magnitude_total, axis='index')\n",
    "    \n",
    "    data_matrix = calculate_similarity(data_items)\n",
    "    \n",
    "    data_neighbours = pd.DataFrame(index=data_matrix.columns, columns=range(1,neighbour+1))\n",
    "    for i in range(0, len(data_matrix.columns)):\n",
    "        data_neighbours.iloc[i,:neighbour] = data_matrix.iloc[0:,i].sort_values(ascending=False)[:neighbour].index\n",
    "        \n",
    "    dfRec = makeRecDictionaryItemBased빈칸(usersList, data, data_items, data_neighbours, data_matrix)\n",
    "    return dfRec\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "dfRec_itembased_nei = recommendItemBased(df_new_사원_집합, sampleUsersList, 50)\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "dfRec_itembased_nei.to_excel(\"itemBasedRec_사원_2010_집합_0709.xlsx\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "def splitingKnownUnknown(testDat, givenNum):\n",
    "    \n",
    "    for i in range(testDat.shape[0]):\n",
    "\n",
    "        user = testDat[i]\n",
    "        allonesarr = np.where(testDat[i] > 0)[0]\n",
    "        sampleonesList = np.random.choice(allonesarr, givenNum, replace = False).tolist()\n",
    "        if i==0:\n",
    "            known = np.zeros(testDat.shape[1])\n",
    "            np.put(known, sampleonesList, 1)\n",
    "            unknown = user - known\n",
    "        else:\n",
    "            known_new = np.zeros(testDat.shape[1])\n",
    "            np.put(known_new, sampleonesList, 1)\n",
    "            known = np.vstack((known, known_new))\n",
    "            unknown_new = user - known_new\n",
    "            unknown = np.vstack((unknown, unknown_new))\n",
    "    \n",
    "    return known, unknown\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "def normalize(data_items, test_index, test_columns):\n",
    "    data_items = pd.DataFrame(data_items, index = test_index, columns = test_columns)\n",
    "    magnitude_total = np.sqrt(np.square(data_items).sum(axis=1))\n",
    "    data_items_normalized = data_items.divide(magnitude_total, axis='index')\n",
    "    return data_items_normalized\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "def predictItemBasedTrainTest(df_raw, given):\n",
    "    data_mat = pd.get_dummies(df_raw.N_COUR).groupby(df_raw.O_REG).apply(max)\n",
    "    data = data_mat.reset_index()\n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "    #splist the data into train and test\n",
    "    data_items_train, data_items_test = train_test_split(data_items, test_size = 0.20, random_state=42)\n",
    "    #test dataset index and columns\n",
    "    test_index = data_items_test.index\n",
    "    test_columns = data_items_test.columns\n",
    "    #split test dataset into known and unknown for testing\n",
    "    test_known, test_unknown = splitingKnownUnknown(data_items_test.values,given)\n",
    "    #normalize the data\n",
    "    test_known_norm = normalize(test_known, test_index, test_columns)\n",
    "    test_unknown_norm = normalize(test_unknown, test_index, test_columns)\n",
    "    magnitude_total = np.sqrt(np.square(data_items_train).sum(axis=1))\n",
    "    data_train_norm = data_items_train.divide(magnitude_total, axis='index')\n",
    "    #calculate Similarity with training dataset\n",
    "    data_matrix = calculate_similarity(data_train_norm)\n",
    "        \n",
    "    #calculate rating matrix(weighted average)\n",
    "    scoreMat = np.matmul(test_known_norm, data_matrix)\n",
    "    scoreMatNumer = data_matrix.sum(axis=1)\n",
    "    scoreMatWeighted = scoreMat.div(scoreMatNumer, axis = 'columns')\n",
    "    \n",
    "#     print(scoreMatWeighted)\n",
    "    \n",
    "    #index of test dataset users' purchase record\n",
    "    knownlikesIndex = np.where(test_known_norm.values!=0)\n",
    "    #drop knownlikesvalues by index\n",
    "    scoreMatWeighted_values = scoreMatWeighted.values\n",
    "    scoreMatWeighted_values[knownlikesIndex] = 0\n",
    "    \n",
    "    scoreMatWeighted_values_df = pd.DataFrame(scoreMatWeighted_values, index = scoreMat.index, columns = scoreMat.columns)\n",
    "    \n",
    "    return scoreMatWeighted_values_df, test_unknown\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def predictItemBasedTrainTestKNN(df_raw, given):\n",
    "    data_mat = pd.get_dummies(df_raw.N_COUR).groupby(df_raw.O_REG).apply(max)\n",
    "    data = data_mat.reset_index()\n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "    #splist the data into train and test\n",
    "    data_items_train, data_items_test = train_test_split(data_items, test_size = 0.20, random_state=42)\n",
    "    #test dataset index and columns\n",
    "    test_index = data_items_test.index\n",
    "    test_columns = data_items_test.columns\n",
    "    #split test dataset into known and unknown for testing\n",
    "    test_known, test_unknown = splitingKnownUnknown(data_items_test.values,given)\n",
    "    #normalize the data\n",
    "    test_known_norm = normalize(test_known, test_index, test_columns)\n",
    "    test_unknown_norm = normalize(test_unknown, test_index, test_columns)\n",
    "    magnitude_total = np.sqrt(np.square(data_items_train).sum(axis=1))\n",
    "    data_train_norm = data_items_train.divide(magnitude_total, axis='index')\n",
    "    #calculate Similarity with training dataset\n",
    "    data_matrix = calculate_similarity(data_train_norm)\n",
    "    \n",
    "    data_neighbours = pd.DataFrame(index=data_matrix.columns, columns=range(1,50+1))\n",
    "    for i in range(0, len(data_matrix.columns)):\n",
    "        data_neighbours.iloc[i,:50] = data_matrix.iloc[0:,i].sort_values(ascending=False)[:50].index\n",
    "    \n",
    "    \n",
    "    \n",
    "    #calculate rating matrix(weighted average)\n",
    "    scoreMat = np.matmul(test_known_norm, data_matrix)\n",
    "    scoreMatNumer = data_matrix.sum(axis=1)\n",
    "    scoreMatWeighted = scoreMat.div(scoreMatNumer, axis = 'columns')\n",
    "    \n",
    "#     print(scoreMatWeighted)\n",
    "    \n",
    "    #index of test dataset users' purchase record\n",
    "    knownlikesIndex = np.where(test_known_norm.values!=0)\n",
    "    #drop knownlikesvalues by index\n",
    "    scoreMatWeighted_values = scoreMatWeighted.values\n",
    "    scoreMatWeighted_values[knownlikesIndex] = 0\n",
    "    \n",
    "    scoreMatWeighted_values_df = pd.DataFrame(scoreMatWeighted_values, index = scoreMat.index, columns = scoreMat.columns)\n",
    "    \n",
    "    return scoreMatWeighted_values_df\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "def evaluateModel(df_score, test_unknown, given, topN):\n",
    "    # df = scoreMat_values_df\n",
    "    #record topN courses as 1 and rest of the courses as 0\n",
    "    #optimize required\n",
    "    df_scoreMat = copy.deepcopy(df_score)\n",
    "    test_unknown_func = test_unknown\n",
    "    for i in range(4774):\n",
    "        df_scoreMat.iloc[i][df_scoreMat.iloc[i].nlargest(topN).index.tolist()] = 1\n",
    "    df_scoreMat[df_scoreMat!=1] = 0\n",
    "    \n",
    "    #calculate metrics\n",
    "    TP = np.sum(np.multiply(df_scoreMat.values, test_unknown_func), axis=1) # element-wise multiplication\n",
    "#     print(TP)\n",
    "    TP_FN = test_unknown_func.sum(axis=1)\n",
    "    TP_FP = df_scoreMat.values.sum(axis=1)\n",
    "#     print(TP_FP)\n",
    "    FP = np.subtract(TP_FP, TP)\n",
    "#     print(FP)\n",
    "    FN = np.subtract(TP_FN, TP)\n",
    "    TN = test_unknown_func.shape[1] - given - TP - FP - FN\n",
    "#     print('given number is', given)\n",
    "#     print('TopN is ', topN)\n",
    "    precision = np.divide(TP,np.add(TP, FP))\n",
    "    recall = np.divide(TP,np.add(TP, FN))\n",
    "    TPR = recall\n",
    "    FPR = np.divide(FP,np.add(FP, TN))\n",
    "    \n",
    "    metrics = [{'precisionAvg' : np.mean(precision), 'recallAvg' : np.mean(recall), 'TPRAvg' : np.mean(TPR), 'FPRAvg' : np.mean(FPR)}]\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "scoreMat, test_unknown_df = predictItemBasedTrainTest(df_NEW_사원, 4)\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "scoreMat\n",
    "\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "\n",
    "metricsdf_1 = evaluateModel(scoreMat, test_unknown_df, 4, 1)\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "metricsdf_1\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "metricsdf_3 = evaluateModel(scoreMat, test_unknown_df, 4, 3)\n",
    "\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "metricsdf_3\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "metricsdf_5 = evaluateModel(scoreMat, test_unknown_df, 4, 5)\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "metricsdf_5\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "metricsdf_10 = evaluateModel(scoreMat, test_unknown_df, 4, 10)\n",
    "\n",
    "\n",
    "# In[81]:\n",
    "\n",
    "\n",
    "metricsdf_10\n",
    "\n",
    "\n",
    "# ## CF - User-Based\n",
    "\n",
    "# In[195]:\n",
    "\n",
    "\n",
    "def calculateUserSim(data_items):\n",
    "    \n",
    "    data_sparse = sparse.csr_matrix(data_items)\n",
    "    similarities = cosine_similarity(data_sparse)\n",
    "    sim = pd.DataFrame(data=similarities, index= data_items.index, columns= data_items.index)\n",
    "    return sim\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def normalize(data_items, test_index, test_columns):\n",
    "    data_items = pd.DataFrame(data_items, index = test_index, columns = test_columns)\n",
    "    magnitude_total = np.sqrt(np.square(data_items).sum(axis=1))\n",
    "    data_items_normalized = data_items.divide(magnitude_total, axis='index')\n",
    "    return data_items_normalized\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def predictUserBasedTrainTest(df, given):\n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    data = data_mat.reset_index()\n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "    #splist the data into train and test\n",
    "    data_items_train, data_items_test = train_test_split(data_items, test_size = 0.20, random_state=42)\n",
    "    #test dataset index and columns\n",
    "    test_index = data_items_test.index\n",
    "    test_columns = data_items_test.columns\n",
    "    #split test dataset into known and unknown for testing\n",
    "    test_known, test_unknown = splitingKnownUnknown(data_items_test.values,given)\n",
    "    #normalize the data\n",
    "    test_known_norm = normalize(test_known, test_index, test_columns)\n",
    "    test_unknown_norm = normalize(test_unknown, test_index, test_columns)\n",
    "    magnitude_total = np.sqrt(np.square(data_items_train).sum(axis=1))\n",
    "    data_train_norm = data_items_train.divide(magnitude_total, axis='index')\n",
    "    #calculate Similarity with training dataset\n",
    "    data_matrix = calculateUserSim(data_train_norm)\n",
    "        \n",
    "    #calculate rating matrix(weighted average)\n",
    "    scoreMat = np.matmul(test_known_norm, data_matrix)\n",
    "    scoreMatNumer = data_matrix.sum(axis=1)\n",
    "    scoreMatWeighted = scoreMat.div(scoreMatNumer, axis = 'columns')\n",
    "    \n",
    "#     print(scoreMatWeighted)\n",
    "    \n",
    "    #index of test dataset users' purchase record\n",
    "    knownlikesIndex = np.where(test_known_norm.values!=0)\n",
    "    #drop knownlikesvalues by index\n",
    "    scoreMatWeighted_values = scoreMatWeighted.values\n",
    "    scoreMatWeighted_values[knownlikesIndex] = 0\n",
    "    \n",
    "    scoreMatWeighted_values_df = pd.DataFrame(scoreMatWeighted_values, index = scoreMat.index, columns = scoreMat.columns)\n",
    "    \n",
    "    return scoreMatWeighted_values_df\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[152]:\n",
    "\n",
    "\n",
    "def makeRecDictionaryUserBased(usersList, df, df_items, user_pred):\n",
    "    userLikesDict_userbased = {}\n",
    "    recommenDict_userbased = {}\n",
    "    for user in usersList:\n",
    "        user_index = df[df.O_REG == user].index.tolist()[0] # Get the frame index\n",
    "\n",
    "        # # Get the artists the user has likd.\n",
    "        known_user_likes = df_items.loc[user_index]\n",
    "        #print(type(known_user_likes))\n",
    "        known_user_likes = known_user_likes[known_user_likes >0].index.values\n",
    "\n",
    "        user_pred_df_user = user_pred.iloc[user_index]\n",
    "\n",
    "        # # Remove the known likes from the recommendation.\n",
    "        user_pred_df_score = user_pred_df_user.drop(known_user_likes)\n",
    "        \n",
    "        known_user_list = known_user_likes.tolist()\n",
    "        \n",
    "#         for i in range(len(coursePreReq_Ser.index)):\n",
    "#             if(coursePreReq_Ser.index[i] in known_user_list):\n",
    "#                 score = score.drop(coursePreReq_Ser[i], errors = 'ignore')\n",
    "        \n",
    "#         if('자금세탁방지 핵심요원(전문)' in known_user_list):\n",
    "#             user_pred_df_score = user_pred_df_score.drop('자금세탁방지 핵심요원(기초)')\n",
    "#         if('여신법률' in known_user_list):\n",
    "#             user_pred_df_score = user_pred_df_score.drop('여신법률(담보관리)기초')\n",
    "\n",
    "        recommenDict_userbased[user] = user_pred_df_score.nlargest(10).index.tolist()\n",
    "        userLikesDict_userbased[user] = known_user_likes.tolist()\n",
    "    \n",
    "    dfRec_userbased = pd.DataFrame(recommenDict_userbased).T\n",
    "    userLikesSeries = pd.Series(userLikesDict_userbased)\n",
    "    dfRec_userbased['userLikes'] = userLikesSeries\n",
    "    \n",
    "    return dfRec_userbased\n",
    "\n",
    "\n",
    "# In[153]:\n",
    "\n",
    "\n",
    "def recommendUserBased(df, usersList):\n",
    "\n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "    #normalize\n",
    "    magnitude_total = np.sqrt(np.square(data_items).sum(axis=1))\n",
    "    data_items = data_items.divide(magnitude_total, axis='index')\n",
    "        \n",
    "    user_pred = calculateUserSim(data_mat)\n",
    "    data = data_mat.reset_index()\n",
    "\n",
    "    dfRec = makeRecDictionaryUserBased(usersList, data, data_items, user_pred)\n",
    "    return dfRec\n",
    "\n",
    "\n",
    "# In[149]:\n",
    "\n",
    "\n",
    "sampleUsersList2 = sampleUserList(df_new_사원_2015, 50, 2)\n",
    "\n",
    "\n",
    "# In[155]:\n",
    "\n",
    "\n",
    "dfRec_userbased = recommendUserBased(df, userList)\n",
    "\n",
    "\n",
    "# In[430]:\n",
    "\n",
    "\n",
    "dfRec_userbased.to_excel(\"userBasedRec_사원_2015.xlsx\")\n",
    "\n",
    "\n",
    "# # Logistic Regression\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[355]:\n",
    "\n",
    "\n",
    "user_item_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "user_item_mat\n",
    "\n",
    "\n",
    "# In[356]:\n",
    "\n",
    "\n",
    "user_item_mat_train, user_item_mat_test = train_test_split(user_item_mat, test_size = 0.15, random_state=42)\n",
    "\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "\n",
    "data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    \n",
    "train_df, test_df = train_test_split(data_mat, test_size = 0.15, random_state=42)\n",
    "    \n",
    "userList = pd.Series(test_df.index).sample(n=50, random_state=2).values.tolist()\n",
    "\n",
    "\n",
    "# In[366]:\n",
    "\n",
    "\n",
    "def recLogistic(df):\n",
    "    \n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    \n",
    "    train_df, test_df = train_test_split(data_mat, test_size = 0.15, random_state=42)\n",
    "    \n",
    "    userList = pd.Series(test_df.index).sample(n=50, random_state=2).values.tolist()\n",
    "    \n",
    "    num_users = len(userList)\n",
    "    num_items = num_features + 1\n",
    "    \n",
    "    user_pred = np.zeros((num_users, num_items))\n",
    "        \n",
    "    for i in range(num_items):\n",
    "        x_train = train_df.drop(train_df.columns[i],axis=1)\n",
    "        y_train = train_df.iloc[:,i]\n",
    "        x_test = test_df.drop(test_df.columns[i],axis=1)\n",
    "        x_test = x_test.loc[userList]\n",
    "#         y_test = test_df.iloc[:,i]\n",
    "        \n",
    "        logmodel = LogisticRegression(solver='lbfgs', max_iter = 200)\n",
    "#         print(i)\n",
    "        logmodel.fit(x_train,y_train)\n",
    "        \n",
    "        purchaseProb = logmodel.predict_proba(x_test)[:,1] # array (#examples, )\n",
    "        \n",
    "#         active_user_features = active_user_ratings.drop(active_user_ratings.columns[i], axis=1)\n",
    "#         active_user_features = active_user_features.values # 10 X 373\n",
    "# #         purchase_prob = sigmoid(np.inner(W, active_user_features)) # 10 X 1\n",
    "        \n",
    "        user_pred[:,i] = purchaseProb\n",
    "        \n",
    "        ### delete known userlikes in each row(user) or make it zero and select most probable items for each row\n",
    "    user_pred_df = pd.DataFrame(user_pred)\n",
    "    user_pred_df.columns = data_mat.columns.tolist()     \n",
    "    \n",
    "    userLikesDict_logistic = {}\n",
    "    recommenDict_logistic = {}\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        \n",
    "        # # Get the artists the user has likd.\n",
    "        useri_history = test_df.loc[[userList[i]]]\n",
    "        known_user_likes = np.where(useri_history == 1)[1].tolist()\n",
    "\n",
    "        user_pred_df_score = user_pred_df.iloc[[i]].drop(user_pred_df.columns[known_user_likes], axis=1)\n",
    "        \n",
    "        recommenDict_logistic[userList[i]] = user_pred_df_score.iloc[0].nlargest(10).index.tolist()\n",
    "        userLikesDict_logistic[userList[i]] = user_pred_df.columns[known_user_likes].values.tolist()\n",
    "    \n",
    "    dfRec_logistic = pd.DataFrame(recommenDict_logistic).T\n",
    "    userLikesSeries = pd.Series(userLikesDict_logistic)\n",
    "    dfRec_logistic['userLikes'] = userLikesSeries\n",
    "    \n",
    "    return dfRec_logistic\n",
    "\n",
    "\n",
    "# In[367]:\n",
    "\n",
    "\n",
    "dfRec_Logistic = recLogistic(df)\n",
    "\n",
    "\n",
    "# In[372]:\n",
    "\n",
    "\n",
    "dfRec_Logistic.to_excel(\"logistic_사원_2010.xlsx\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[381]:\n",
    "\n",
    "\n",
    "num_features = user_item_mat_values.shape[1] - 1\n",
    "w = np.zeros(num_features)\n",
    "N = user_item_mat_train.shape[0] #number of training examples\n",
    "\n",
    "\n",
    "# In[139]:\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# In[140]:\n",
    "\n",
    "\n",
    "def loss(x,y,w):\n",
    "    loss=0\n",
    "    for i in range(N):\n",
    "        loss += -y[0,i]*np.log(sigmoid(np.dot(w.T, x[:,i]))) - (1 - y[0,i])*np.log(1 - np.dot(w.T, x[:,i]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# In[141]:\n",
    "\n",
    "\n",
    "def gradient(x,y,w):\n",
    "    lamda = 10\n",
    "    gradient = np.zeros(num_features)\n",
    "    for i in range(N):\n",
    "        D = sigmoid(np.dot(w.T,x[:,i])) - y[0,i]\n",
    "        G = D * x[:,i]\n",
    "        gradient += G\n",
    "    gradient += 2*lamda*w\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# In[142]:\n",
    "\n",
    "\n",
    "def hessian(x,y,w):\n",
    "    lamda = 10\n",
    "    hessian = np.zeros((num_features,num_features))\n",
    "    I = np.identity(num_features)\n",
    "    for i in range(N):\n",
    "        xxt = np.outer(x[:,i],x[:,i])\n",
    "        sigmoidsquared = sigmoid(np.dot(w.T,x[:,i]))*(1 - sigmoid(np.dot(w.T, x[:,i])))\n",
    "        H = xxt * sigmoidsquared\n",
    "        hessian += H\n",
    "    hessian += 2*lamda*I\n",
    "    return hessian\n",
    "\n",
    "\n",
    "# In[143]:\n",
    "\n",
    "\n",
    "def Newton(x, y, tol):\n",
    "    \n",
    "    w = np.zeros(num_features)\n",
    "    lamda = 10\n",
    "    error = 10\n",
    "    \n",
    "    while error>tol :\n",
    "        \n",
    "        l1_train = loss( x, y, w)\n",
    "        norm = np.linalg.norm(w)\n",
    "        l1_train+= lamda*(norm**2)\n",
    "        \n",
    "        G = gradient(x, y, w)\n",
    "        \n",
    "        H = hessian(x, y, w)\n",
    "        \n",
    "        w = w - np.dot(np.linalg.pinv(H), G)\n",
    "        \n",
    "        l2_train = loss(x, y, w)\n",
    "        norm = np.linalg.norm(w)\n",
    "        l2_train += lamda*(norm**2)\n",
    "        error = np.abs(l2_train - l1_train)\n",
    "        \n",
    "    return w\n",
    "\n",
    "\n",
    "# In[388]:\n",
    "\n",
    "\n",
    "def recLogisticMyOwn(df):\n",
    "    \n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    \n",
    "    train_df, test_df = train_test_split(data_mat, test_size = 0.15, random_state=42)   \n",
    "    userList = pd.Series(test_df.index).sample(n=50, random_state=2).values.tolist()\n",
    "    \n",
    "    num_users = len(userList)\n",
    "    num_items = num_features + 1\n",
    "    \n",
    "    user_pred = np.zeros((num_users, num_items))\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        x_train = train_df.drop(train_df.columns[i],axis=1)\n",
    "#         x_train = x_train.values\n",
    "        x_train = x_train.T\n",
    "        y_train = train_df.iloc[:,i]\n",
    "        y_train = y_train.T\n",
    "        x_test = test_df.drop(test_df.columns[i],axis=1)\n",
    "        x_test = x_test.loc[userList]\n",
    "        x_test = x_test.T\n",
    "#         x_test = np.delete(user_item_mat_test, i, axis=1)\n",
    "#         x_test = x_test.T\n",
    "#         y_test = user_item_mat_test[:,i]\n",
    "#         y_test = y_test.reshape(-1,1)\n",
    "#         y_test = y_test.T\n",
    "\n",
    "        W = Newton(x_train.values, y_train.values, 10**(-6)) # 1 X 373\n",
    "\n",
    "        \n",
    "#         active_user_features = active_user_ratings.drop(active_user_ratings.columns[i], axis=1)\n",
    "#         active_user_features = active_user_features.values # 10 X 373\n",
    "        purchase_prob = sigmoid(np.inner(W, x_test.values)) # 10 X 1\n",
    "        \n",
    "        user_pred[:,i] = purchase_prob\n",
    "        \n",
    "        ### delete known userlikes in each row(user) or make it zero and select most probable items for each row\n",
    "    user_pred_df = pd.DataFrame(user_pred)\n",
    "    user_pred_df.columns = df.columns.tolist()     \n",
    "    \n",
    "    userLikesDict_logistic = {}\n",
    "    recommenDict_logistic = {}\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        \n",
    "        # # Get the artists the user has likd.\n",
    "        useri_history = test_df.loc[[userList[i]]]\n",
    "        known_user_likes = np.where(useri_history == 1)[1].tolist()\n",
    "\n",
    "        user_pred_df_score = user_pred_df.iloc[[i]].drop(user_pred_df.columns[known_user_likes], axis=1)\n",
    "        \n",
    "        recommenDict_logistic[userList[i]] = user_pred_df_score.iloc[0].nlargest(10).index.tolist()\n",
    "        userLikesDict_logistic[userList[i]] = user_pred_df.columns[known_user_likes].values.tolist()\n",
    "    \n",
    "    dfRec_logistic = pd.DataFrame(recommenDict_logistic).T\n",
    "    userLikesSeries = pd.Series(userLikesDict_logistic)\n",
    "    dfRec_logistic['userLikes'] = userLikesSeries\n",
    "    \n",
    "    return dfRec_logistic\n",
    "        \n",
    "\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "\n",
    "def recLogistic(df, userList):\n",
    "    \n",
    "    num_users = len(userList)\n",
    "    num_items = num_features + 1\n",
    "    \n",
    "    user_pred = np.zeros((num_users, num_items))\n",
    "    \n",
    "    data_mat = pd.get_dummies(df.N_COUR).groupby(df.O_REG).apply(max)\n",
    "    \n",
    "    data = data_mat.reset_index()\n",
    "    \n",
    "    data_items = data_mat.reset_index(drop=True)\n",
    "    data_items_values = data_items.values\n",
    "    user_item_mat_train, user_item_mat_test = train_test_split(data_items_values, test_size = 0.15, random_state=42) #numpy ndarray\n",
    "    \n",
    "    user_index_list = []\n",
    "    for user in userList:\n",
    "        user_index = data[data.O_REG == user].index.tolist()[0]\n",
    "        user_index_list.append(user_index)\n",
    "        \n",
    "    active_user_ratings = data_items.iloc[user_index_list,:] #dataframe\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        x_train = np.delete(user_item_mat_train, i, axis=1) ## #users X #features\n",
    "        x_train = x_train.T  ## #features X #users\n",
    "        y_train = user_item_mat_train[:,i]\n",
    "        y_train = y_train.reshape(-1,1) ## #users X 1\n",
    "        y_train = y_train.T  ## 1 X #users\n",
    "#         x_test = np.delete(user_item_mat_test, i, axis=1)\n",
    "#         x_test = x_test.T\n",
    "#         y_test = user_item_mat_test[:,i]\n",
    "#         y_test = y_test.reshape(-1,1)\n",
    "#         y_test = y_test.T\n",
    "\n",
    "        W = Newton(x_train, y_train, 10**(-6)) # 1 X 373\n",
    "\n",
    "        \n",
    "        active_user_features = active_user_ratings.drop(active_user_ratings.columns[i], axis=1)\n",
    "        active_user_features = active_user_features.values # 10 X 373\n",
    "        purchase_prob = sigmoid(np.inner(W, active_user_features)) # 10 X 1\n",
    "        \n",
    "        user_pred[:,i] = purchase_prob\n",
    "        \n",
    "        ### delete known userlikes in each row(user) or make it zero and select most probable items for each row\n",
    "    user_pred_df = pd.DataFrame(user_pred)\n",
    "    user_pred_df.columns = df.columns.tolist()     \n",
    "    \n",
    "    userLikesDict_logistic = {}\n",
    "    recommenDict_logistic = {}\n",
    "    \n",
    "    for i in range(num_users):\n",
    "        \n",
    "        # # Get the artists the user has likd.\n",
    "        known_user_likes = data_items.loc[user_index_list[i]]\n",
    "        known_user_likes = known_user_likes[known_user_likes >0].index.values #numpy ndarray\n",
    "\n",
    "        user_pred_df_score = user_pred_df.iloc[i].drop(known_user_likes)\n",
    "        \n",
    "        recommenDict_logistic[userList[i]] = user_pred_df_score.nlargest(10).index.tolist()\n",
    "        userLikesDict_logistic[userList[i]] = known_user_likes.tolist()\n",
    "    \n",
    "    dfRec_logistic = pd.DataFrame(recommenDict_logistic).T\n",
    "    userLikesSeries = pd.Series(userLikesDict_logistic)\n",
    "    dfRec_logistic['userLikes'] = userLikesSeries\n",
    "    \n",
    "    return dfRec_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
